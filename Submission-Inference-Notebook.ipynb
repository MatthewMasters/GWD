{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp '../input/weightedboxesfusion' . -r\n",
    "!pip install --no-deps './weightedboxesfusion' > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fba275e8e30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import numba\n",
    "import re\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from shutil import copy\n",
    "from os.path import join, exists\n",
    "from numba import jit\n",
    "from typing import List, Union, Tuple\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch import LongTensor as LongTensor\n",
    "from torch import FloatTensor as FloatTensor\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision\n",
    "from torchvision.models import resnet\n",
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "from torchvision.models.detection.faster_rcnn import TwoMLPHead, FastRCNNPredictor\n",
    "from torchvision.models.detection.rpn import AnchorGenerator, RPNHead, RegionProposalNetwork\n",
    "from torchvision.models.detection.roi_heads import RoIHeads\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork, LastLevelMaxPool\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision.ops.misc import FrozenBatchNorm2d\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from ensemble_boxes import *\n",
    "\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 4\n",
    "PL_ROUNDS = 1\n",
    "PL_EPOCHS = 5\n",
    "detection_threshold = 0.5\n",
    "image_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/kaggle/input/global-wheat-detection'\n",
    "original_train_path = '/kaggle/input/original-train/original_train'\n",
    "# weights_path = '/kaggle/input/1024x1024-1/weights_e149.pth'\n",
    "weights_path = '/kaggle/input/manual-weights-7/e152.pth'\n",
    "train_path = join(dataset_path, 'train')\n",
    "test_path = join(dataset_path, 'test')\n",
    "train_df = pd.read_csv(join(original_train_path, 'train.csv'))\n",
    "train_df['image_path'] = [join(original_train_path, s, i + '.jpeg') for i, s in train_df[['image_id', 'source']].values]\n",
    "test_df = pd.read_csv(join(dataset_path, 'sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split validation from training\n",
    "# Take 30 examples from each source\n",
    "# valid_ids = []\n",
    "# for source in pd.unique(train_df['source']):\n",
    "#     if source == 'test': continue\n",
    "#     source_ids = train_df[train_df['source'] == source]['image_id'].values\n",
    "#     valid_ids.extend(list(np.random.choice(source_ids, 50, replace=False)))\n",
    "\n",
    "# valid_ids = [p.split('/')[-1].split('.')[0] for p in glob('/kaggle/input/global-wheat-detection/test/*')[::10]]\n",
    "# print(valid_ids)\n",
    "# print(len(valid_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_path = {}\n",
    "for idx, row in train_df.iterrows():\n",
    "    image_id = row['image_id']\n",
    "    if image_id in id_to_path.keys(): continue\n",
    "    id_to_path[image_id] = row['image_path']\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    image_id = row['image_id']\n",
    "    if image_id in id_to_path.keys(): continue\n",
    "    id_to_path[image_id] = join(test_path, image_id + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_id', 'width', 'height', 'source', 'x', 'y', 'w', 'h', 'x1', 'y1', 'image_id_orig', 'image_path']\n"
     ]
    }
   ],
   "source": [
    "print(list(train_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Dataset functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WheatTrainDataset(Dataset):\n",
    "    def __init__(self, dataframe, transforms=None, test=False):\n",
    "        super().__init__()\n",
    "        self.df = dataframe\n",
    "        self.image_ids = pd.unique(dataframe['image_id'])\n",
    "        self.image_paths = [id_to_path[image_id] for image_id in self.image_ids]\n",
    "        self.length = len(self.image_ids)\n",
    "        self.transforms = transforms\n",
    "        self.test = test\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        image_path = self.image_paths[index]\n",
    "        image = self.load_image(image_path) \n",
    "        boxes = self.load_boxes(image_id)\n",
    "\n",
    "        # if not self.test and random.random() > 0.5:\n",
    "        #     image, boxes = self.cutmix_image_and_boxes(image, boxes)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([index])}\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = {'image': image, 'bboxes': target['boxes'], 'labels': labels}\n",
    "            sample = self.transforms(**sample)\n",
    "            image, boxes = sample['image'], sample['bboxes']\n",
    "            boxes = self.filter_boxes(boxes)\n",
    "            if len(boxes):\n",
    "                target['boxes'] = torch.stack([torch.tensor(box, dtype=torch.float32) for box in zip(*boxes)]).permute(1, 0)\n",
    "            else:\n",
    "                return self.__getitem__(np.random.randint(self.length))\n",
    "                # target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                # target['labels'] = torch.zeros(0, dtype=torch.int64)\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "    def load_image(self, image_path):\n",
    "        image = cv2.imread(image_path , cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        return image\n",
    "\n",
    "    def load_boxes(self, image_id):\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "        if 'x1' in records.columns and 'y1' in records.columns:\n",
    "            return records[['x', 'y', 'x1', 'y1']].values\n",
    "        else:\n",
    "            boxes = records[['x', 'y', 'w', 'h']].values\n",
    "            boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "            boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "            return boxes\n",
    "\n",
    "    def filter_boxes(self, boxes):\n",
    "        min_length = 13\n",
    "        min_area = 400\n",
    "        max_area = 145360\n",
    "        max_length_ratio = 18\n",
    "        min_length_ratio = 1.0/max_length_ratio\n",
    "\n",
    "        boxes_out = []\n",
    "        for box in boxes:\n",
    "            x, y, x1, y1 = box\n",
    "            w = x1 - x\n",
    "            if w < min_length: continue\n",
    "            h = y1 - y\n",
    "            if h < min_length: continue\n",
    "            area = w * h\n",
    "            if area < 400 or area > 145360: continue\n",
    "            length_ratio = w / h\n",
    "            if length_ratio < min_length_ratio or length_ratio > max_length_ratio: continue\n",
    "            boxes_out.append(box)\n",
    "\n",
    "        return boxes_out\n",
    "\n",
    "    def get_sample_weights(self):\n",
    "        weights = []\n",
    "        for image_id in self.image_ids:\n",
    "            w, h, source = self.df[self.df['image_id'] == image_id][['width', 'height', 'source']].values[0]\n",
    "            if w == 3072:\n",
    "                # 2x3\n",
    "                weight = 8\n",
    "            elif h == 2048:\n",
    "                # 2x2\n",
    "                weight = 5\n",
    "            elif w == 2048:\n",
    "                # 1x2\n",
    "                weight = 2.5\n",
    "            else:\n",
    "                # 1x1\n",
    "                weight = 1\n",
    "            if source == 'test': weight *= 10\n",
    "            weights.append(weight)\n",
    "        return np.array(weights)\n",
    "\n",
    "    \n",
    "class WheatTestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, image_dir, image_size=1024, onfly=False, tta=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.image_size = image_size\n",
    "        self.onfly = onfly\n",
    "        self.tta = tta\n",
    "#         if onfly:\n",
    "#             self.load_all_images()\n",
    "\n",
    "#     def load_all_images(self):\n",
    "#         self.images = []\n",
    "#         for image_id in self.image_ids:\n",
    "            \n",
    "        \n",
    "    def load_image(self, image_id):\n",
    "        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (self.image_size, self.image_size))\n",
    "        image = image.astype(np.float32)\n",
    "        image /= 255.0\n",
    "        return image\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = self.load_image(image_id)\n",
    "        if self.tta:\n",
    "            all_images = apply_tta(image)\n",
    "            return all_images, image_id\n",
    "        else:\n",
    "            return image, image_id\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "\n",
    "def collate_fn(batch): return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Transforms & Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wbf(predictions, image_size=1024, iou_thr=0.4, skip_box_thr=0.7, weights=None):\n",
    "    boxes = [pred['boxes'].data.cpu().numpy() for pred in predictions]\n",
    "    boxes = revert_tta(boxes, image_size)\n",
    "    boxes = [box_set/(image_size-1) for box_set in boxes]\n",
    "    scores = [pred['scores'].data.cpu().numpy() for pred in predictions]\n",
    "    labels = [np.ones(pred['scores'].shape[0]) for pred in predictions]\n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels\n",
    "\n",
    "# def ensemble_wbf(boxes, scores, image_size=1024, iou_thr=0.35, skip_box_thr=0.65, weights=None):\n",
    "#     boxes = [box_set/(image_size-1) for box_set in boxes]\n",
    "#     labels = [np.ones(len(score)) for score in scores]\n",
    "#     boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "#     boxes = boxes*(image_size-1)\n",
    "#     return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.RandomCrop(1024, 1024),\n",
    "            A.RandomSizedCrop(min_max_height=(800, 800), height=1024, width=1024, p=0.5),\n",
    "            A.OneOf([\n",
    "                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.9),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.9),\n",
    "            ], p=0.9),\n",
    "            A.ToGray(p=0.01),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0,\n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.RandomCrop(1024, 1024),\n",
    "#             A.RandomSizedCrop(min_max_height=(800, 800), height=1024, width=1024, p=0.5),\n",
    "#             A.HorizontalFlip(p=0.5),\n",
    "#             A.VerticalFlip(p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0,\n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "def apply_tta(image):\n",
    "    \"\"\"Apply Test Time Augmentation (TTA)\"\"\"\n",
    "    flipV_sample = flip_v_image(image)\n",
    "    flipH_sample = flip_h_image(image)\n",
    "    flipVH_sample = flip_h_image(flip_v_image(image))\n",
    "    rot1_sample = np.rot90(image)\n",
    "    rot2_sample = np.rot90(rot1_sample)\n",
    "    rot3_sample = np.rot90(rot2_sample)\n",
    "    zoom = zoom_image(image)\n",
    "    zoom_rot1 = np.rot90(zoom)\n",
    "    zoom_rot2 = np.rot90(zoom_rot1)\n",
    "    zoom_rot3 = np.rot90(zoom_rot2)\n",
    "    return [image, flipV_sample, flipH_sample, flipVH_sample, rot1_sample, rot2_sample, rot3_sample, zoom, zoom_rot1, zoom_rot2, zoom_rot3]\n",
    "\n",
    "def revert_tta(boxes, img_size):\n",
    "    \"\"\"Undo TTA in order to ensemble predictions\"\"\"\n",
    "    sample0, flippedV, flippedH, flippedVH, rot1, rot2, rot3, zoomed, zoomed_rot1, zoomed_rot2, zoomed_rot3 = boxes\n",
    "    sample1 = flip_v_boxes(flippedV, img_size)\n",
    "    sample2 = flip_h_boxes(flippedH, img_size)\n",
    "    sample3 = flip_v_boxes(flip_h_boxes(flippedVH, img_size), img_size)\n",
    "    sample4 = rotate_boxes(rot1, img_size, 3)\n",
    "    sample5 = rotate_boxes(rot2, img_size, 2)\n",
    "    sample6 = rotate_boxes(rot3, img_size, 1)\n",
    "    sample7 = unzoom_boxes(zoomed)\n",
    "    sample8 = unzoom_boxes(rotate_boxes(zoomed_rot1, img_size, 3))\n",
    "    sample9 = unzoom_boxes(rotate_boxes(zoomed_rot2, img_size, 2))\n",
    "    sample10 = unzoom_boxes(rotate_boxes(zoomed_rot3, img_size, 1))\n",
    "    return [sample0, sample1, sample2, sample3, sample4, sample5, sample6, sample7, sample8, sample9, sample10]\n",
    "\n",
    "# def apply_tta(image):\n",
    "#     \"\"\"Apply Test Time Augmentation (TTA)\"\"\"\n",
    "#     flipV_sample = flip_v_image(image)\n",
    "#     return [image, flipV_sample]\n",
    "\n",
    "# def revert_tta(boxes, img_size):\n",
    "#     \"\"\"Undo TTA in order to ensemble predictions\"\"\"\n",
    "#     sample0, flippedV = boxes\n",
    "#     sample1 = flip_v_boxes(flippedV, img_size)\n",
    "#     return [sample0, sample1]\n",
    "\n",
    "def zoom_image(image):\n",
    "    zoom = np.zeros_like(image)\n",
    "    zoom[100:900, 100:900] = cv2.resize(image, (800, 800))\n",
    "    return zoom.astype(np.float32)\n",
    "\n",
    "def unzoom_boxes(boxes):\n",
    "    return (boxes - 100) * 1.28\n",
    "\n",
    "def flip_v_image(image):\n",
    "    return np.flip(image, axis=0)\n",
    "    \n",
    "def flip_h_image(image):\n",
    "    return np.flip(image, axis=1)\n",
    "\n",
    "def rotate_boxes(boxes, img_size, k):\n",
    "    for _ in range(k):\n",
    "        x0 = boxes[:, 1]\n",
    "        y0 = img_size - boxes[:, 2]\n",
    "        x1 = boxes[:, 3]\n",
    "        y1 = img_size - boxes[:, 0]\n",
    "        boxes = np.stack([x0, y0, x1, y1], axis=1)\n",
    "    return boxes\n",
    "\n",
    "def flip_v_boxes(boxes, img_size):\n",
    "    y0 = img_size - boxes[:, 3]\n",
    "    y1 = img_size - boxes[:, 1]\n",
    "    boxes[:, 1] = y0\n",
    "    boxes[:, 3] = y1\n",
    "    return boxes\n",
    "    \n",
    "def flip_h_boxes(boxes, img_size):\n",
    "    x0 = img_size - boxes[:, 2]\n",
    "    x1 = img_size - boxes[:, 0]\n",
    "    boxes[:, 0] = x0\n",
    "    boxes[:, 2] = x1\n",
    "    return boxes\n",
    "\n",
    "def tensor_transform(sample):\n",
    "    transform = A.Compose([ToTensorV2(p=1.0)], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "    sample = transform(**sample)\n",
    "    return sample\n",
    "\n",
    "def resize_transform(sample, image_size=1024):\n",
    "    transform = A.Compose([A.Resize(height=image_size, width=image_size, p=1.0)], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "    sample = transform(**sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Metric Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def calculate_iou(gt, pr, form='pascal_voc') -> float:\n",
    "    \"\"\"Calculates the Intersection over Union.\n",
    "\n",
    "    Args:\n",
    "        gt: (np.ndarray[Union[int, float]]) coordinates of the ground-truth box\n",
    "        pr: (np.ndarray[Union[int, float]]) coordinates of the prdected box\n",
    "        form: (str) gt/pred coordinates format\n",
    "            - pascal_voc: [xmin, ymin, xmax, ymax]\n",
    "            - coco: [xmin, ymin, w, h]\n",
    "    Returns:\n",
    "        (float) Intersection over union (0.0 <= iou <= 1.0)\n",
    "    \"\"\"\n",
    "    if form == 'coco':\n",
    "        gt = gt.copy()\n",
    "        pr = pr.copy()\n",
    "\n",
    "        gt[2] = gt[0] + gt[2]\n",
    "        gt[3] = gt[1] + gt[3]\n",
    "        pr[2] = pr[0] + pr[2]\n",
    "        pr[3] = pr[1] + pr[3]\n",
    "\n",
    "    # Calculate overlap area\n",
    "    dx = min(gt[2], pr[2]) - max(gt[0], pr[0]) + 1\n",
    "    \n",
    "    if dx < 0:\n",
    "        return 0.0\n",
    "    \n",
    "    dy = min(gt[3], pr[3]) - max(gt[1], pr[1]) + 1\n",
    "\n",
    "    if dy < 0:\n",
    "        return 0.0\n",
    "\n",
    "    overlap_area = dx * dy\n",
    "\n",
    "    # Calculate union area\n",
    "    union_area = (\n",
    "            (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1) +\n",
    "            (pr[2] - pr[0] + 1) * (pr[3] - pr[1] + 1) -\n",
    "            overlap_area\n",
    "    )\n",
    "\n",
    "    return overlap_area / union_area\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def find_best_match(gts, pred, pred_idx, threshold = 0.5, form = 'pascal_voc', ious=None) -> int:\n",
    "    \"\"\"Returns the index of the 'best match' between the\n",
    "    ground-truth boxes and the prediction. The 'best match'\n",
    "    is the highest IoU. (0.0 IoUs are ignored).\n",
    "\n",
    "    Args:\n",
    "        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n",
    "        pred: (List[Union[int, float]]) Coordinates of the predicted box\n",
    "        pred_idx: (int) Index of the current predicted box\n",
    "        threshold: (float) Threshold\n",
    "        form: (str) Format of the coordinates\n",
    "        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n",
    "\n",
    "    Return:\n",
    "        (int) Index of the best match GT box (-1 if no match above threshold)\n",
    "    \"\"\"\n",
    "    best_match_iou = -np.inf\n",
    "    best_match_idx = -1\n",
    "\n",
    "    for gt_idx in range(len(gts)):\n",
    "        \n",
    "        if gts[gt_idx][0] < 0:\n",
    "            # Already matched GT-box\n",
    "            continue\n",
    "        \n",
    "        iou = -1 if ious is None else ious[gt_idx][pred_idx]\n",
    "\n",
    "        if iou < 0:\n",
    "            iou = calculate_iou(gts[gt_idx], pred, form=form)\n",
    "            \n",
    "            if ious is not None:\n",
    "                ious[gt_idx][pred_idx] = iou\n",
    "\n",
    "        if iou < threshold:\n",
    "            continue\n",
    "\n",
    "        if iou > best_match_iou:\n",
    "            best_match_iou = iou\n",
    "            best_match_idx = gt_idx\n",
    "\n",
    "    return best_match_idx\n",
    "\n",
    "@jit(nopython=True)\n",
    "def calculate_precision(gts, preds, threshold = 0.5, form = 'coco', ious=None) -> float:\n",
    "    \"\"\"Calculates precision for GT - prediction pairs at one threshold.\n",
    "\n",
    "    Args:\n",
    "        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n",
    "        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n",
    "               sorted by confidence value (descending)\n",
    "        threshold: (float) Threshold\n",
    "        form: (str) Format of the coordinates\n",
    "        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n",
    "\n",
    "    Return:\n",
    "        (float) Precision\n",
    "    \"\"\"\n",
    "    n = len(preds)\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    # for pred_idx, pred in enumerate(preds_sorted):\n",
    "    for pred_idx in range(n):\n",
    "\n",
    "        best_match_gt_idx = find_best_match(gts, preds[pred_idx], pred_idx,\n",
    "                                            threshold=threshold, form=form, ious=ious)\n",
    "\n",
    "        if best_match_gt_idx >= 0:\n",
    "            # True positive: The predicted box matches a gt box with an IoU above the threshold.\n",
    "            tp += 1\n",
    "            # Remove the matched GT box\n",
    "            gts[best_match_gt_idx] = -1\n",
    "\n",
    "        else:\n",
    "            # No match\n",
    "            # False positive: indicates a predicted box had no associated gt box.\n",
    "            fp += 1\n",
    "\n",
    "    # False negative: indicates a gt box had no associated predicted box.\n",
    "    fn = (gts.sum(axis=1) > 0).sum()\n",
    "\n",
    "    return tp / (tp + fp + fn)\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def calculate_image_precision(gts, preds, thresholds = (0.5, ), form = 'coco') -> float:\n",
    "    \"\"\"Calculates image precision.\n",
    "\n",
    "    Args:\n",
    "        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n",
    "        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n",
    "               sorted by confidence value (descending)\n",
    "        thresholds: (float) Different thresholds\n",
    "        form: (str) Format of the coordinates\n",
    "\n",
    "    Return:\n",
    "        (float) Precision\n",
    "    \"\"\"\n",
    "    n_threshold = len(thresholds)\n",
    "    image_precision = 0.0\n",
    "    \n",
    "    ious = np.ones((len(gts), len(preds))) * -1\n",
    "    # ious = None\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        precision_at_threshold = calculate_precision(gts.copy(), preds, threshold=threshold,\n",
    "                                                     form=form, ious=ious)\n",
    "        image_precision += precision_at_threshold / n_threshold\n",
    "\n",
    "    return image_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet():\n",
    "    backbone = resnet.__dict__['resnet50'](pretrained=False, norm_layer=FrozenBatchNorm2d)\n",
    "    for name, parameter in backbone.named_parameters():\n",
    "        if 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
    "            parameter.requires_grad_(False)\n",
    "    return backbone\n",
    "\n",
    "class MyFasterRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyFasterRCNN, self).__init__()\n",
    "        image_mean = [0.485, 0.456, 0.406]\n",
    "        image_std = [0.229, 0.224, 0.225]\n",
    "        self.transform = GeneralizedRCNNTransform(1024, 1024, image_mean, image_std)\n",
    "        self.backbone = get_resnet()\n",
    "        return_layers = {'layer1': '0', 'layer2': '1', 'layer3': '2', 'layer4': '3'}\n",
    "        self.body = IntermediateLayerGetter(self.backbone, return_layers=return_layers)\n",
    "\n",
    "        # Feature Pyramid Network\n",
    "        out_channels = 256\n",
    "        in_channels_list = [256 * (2 ** i) for i in range(4)]\n",
    "        self.fpn = FeaturePyramidNetwork(\n",
    "            in_channels_list=in_channels_list,\n",
    "            out_channels=out_channels,\n",
    "            extra_blocks=LastLevelMaxPool())\n",
    "\n",
    "        # Regional Proposal Network\n",
    "        anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "        aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "        anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n",
    "        head = RPNHead(out_channels, anchor_generator.num_anchors_per_location()[0])\n",
    "        self.rpn = RegionProposalNetwork(\n",
    "            anchor_generator=anchor_generator,\n",
    "            head=head,\n",
    "            fg_iou_thresh=0.7,\n",
    "            bg_iou_thresh=0.3,\n",
    "            batch_size_per_image=256,\n",
    "            positive_fraction=0.5,\n",
    "            pre_nms_top_n=dict(training=2000, testing=1000),\n",
    "            post_nms_top_n=dict(training=2000, testing=1000),\n",
    "            nms_thresh=0.7)\n",
    "\n",
    "        # RoI heads\n",
    "        representation_size = 512\n",
    "        box_roi_pool = MultiScaleRoIAlign(['0', '1', '2', '3'], 7, 2)\n",
    "        box_head = TwoMLPHead(out_channels * box_roi_pool.output_size[0] ** 2, representation_size)\n",
    "        box_predictor = FastRCNNPredictor(representation_size, num_classes=2)\n",
    "        self.roi_heads = RoIHeads(\n",
    "            box_roi_pool=box_roi_pool,\n",
    "            box_head=box_head,\n",
    "            box_predictor=box_predictor,\n",
    "            fg_iou_thresh=0.5, bg_iou_thresh=0.5,\n",
    "            batch_size_per_image=512, positive_fraction=0.25,\n",
    "            bbox_reg_weights=None,\n",
    "            score_thresh=0.05,\n",
    "            nms_thresh=0.5,\n",
    "            detections_per_img=100)\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        images, targets = self.transform(images, targets)\n",
    "        features = self.body(images.tensors)\n",
    "\n",
    "        fpn_features = self.fpn(features)\n",
    "        if isinstance(fpn_features, torch.Tensor): fpn_features = OrderedDict([('0', fpn_features)])\n",
    "        proposals, proposal_losses = self.rpn(images, fpn_features, targets)\n",
    "        detections, detector_losses = self.roi_heads(fpn_features, proposals, images.image_sizes, targets)\n",
    "\n",
    "        losses = {}\n",
    "        losses.update(detector_losses)\n",
    "        losses.update(proposal_losses)\n",
    "\n",
    "        return features, detections, losses\n",
    "    \n",
    "\n",
    "class ResNetFeaturizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetFeaturizer, self).__init__()\n",
    "        image_mean = [0.485, 0.456, 0.406]\n",
    "        image_std = [0.229, 0.224, 0.225]\n",
    "        self.transform = GeneralizedRCNNTransform(1024, 1024, image_mean, image_std)\n",
    "        self.backbone = get_resnet()\n",
    "        self.backbone.load_state_dict(torch.load('/kaggle/input/pretrained-pytorch/resnet50-19c8e357.pth'))\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        images, targets = self.transform(images, targets)\n",
    "        features = self.backbone(images.tensors) # images.tensors\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(weights_path):\n",
    "    model = MyFasterRCNN()\n",
    "    num_classes = 2  # 1 class (wheat) + background\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "    model = model.cuda()\n",
    "    model = model.eval()\n",
    "    return model\n",
    "\n",
    "model = get_model(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_test_predictions():\n",
    "    for images, image_ids in test_data_loader:\n",
    "        images = [torch.tensor(image).cuda().permute(2,0,1) for image in images[0]]\n",
    "        features, predictions, loss = model(images)\n",
    "        break\n",
    "\n",
    "    sample = images[0].permute(1,2,0).cpu().numpy()\n",
    "    boxes, scores, labels = run_wbf(predictions, image_size=1024)\n",
    "    boxes = boxes.astype(np.int32)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "    for box in boxes:\n",
    "        cv2.rectangle(sample,\n",
    "                      (box[0], box[1]),\n",
    "                      (box[2], box[3]),\n",
    "                      (220, 0, 0), 2)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = WheatTestDataset(test_df, test_path, tta=False)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4, drop_last=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_test_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_test_data():\n",
    "    rn_model = ResNetFeaturizer().cuda().eval()\n",
    "    \n",
    "    feature_vecs = []\n",
    "    image_ids = []\n",
    "    for images, image_id in tqdm(test_data_loader):\n",
    "        inp = [torch.tensor(images[0]).cuda().permute(2,0,1)]\n",
    "        fv = rn_model(inp)\n",
    "        feature_vecs.append(fv.detach().cpu().numpy())\n",
    "        image_ids.append(image_id[0])\n",
    "    feature_vecs = np.squeeze(np.array(feature_vecs))\n",
    "    print(feature_vecs.shape)\n",
    "    \n",
    "    agglo = AgglomerativeClustering(n_clusters=N_CLUSTERS)\n",
    "    clusters = agglo.fit_predict(feature_vecs)\n",
    "    \n",
    "#     data = [[i,j] for i,j in ]\n",
    "#     cluster_df = pd.DataFrame(clusters, columns=['cluster_id'])\n",
    "    id_to_cluster = {k : v for k,v in zip(image_ids, clusters)}\n",
    "    \n",
    "    del rn_model\n",
    "    del inp\n",
    "    del fv\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return id_to_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "id_to_cluster = cluster_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cluster_id in range(N_CLUSTERS):\n",
    "#     for img_id in cluster_df[cluster_df['cluster_id'] == cluster_id]['image_id'].values:\n",
    "#         img = test_dataset.load_image(img_id)\n",
    "#         plt.imshow(img)\n",
    "#         plt.title('%d' % cluster_id)\n",
    "#         plt.show()\n",
    "\n",
    "# test_df = pd.concat([test_df, cluster_df], axis=1)\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = WheatTestDataset(test_df, test_path)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4, drop_last=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pseudo_labels(combine_with_train=True):\n",
    "    data = []\n",
    "    for images, image_ids in test_data_loader:\n",
    "        predictions = []\n",
    "        for img in images[0]:\n",
    "            inp = [torch.tensor(img).cuda().permute(2,0,1)]\n",
    "            features, pred, loss = model(inp)\n",
    "            predictions.extend(pred)\n",
    "\n",
    "        boxes, scores, labels = run_wbf(predictions, image_size=image_size)\n",
    "        boxes = (boxes).astype(np.int32).clip(min=0, max=int(image_size-1))\n",
    "        image_id = image_ids[0]\n",
    "\n",
    "        for box in boxes:\n",
    "            x0, y0, x1, y1 = box\n",
    "            w = x1 - x0\n",
    "            h = y1 - y0\n",
    "            data.append([image_id, 1024, 1024, 'test', x0, y0, w, h, x1, y1, image_id, join(test_path, image_id + '.jpg'), id_to_cluster[image_id]])\n",
    "    s1_df = pd.DataFrame(data, columns=['image_id', 'width', 'height', 'source', 'x', 'y', 'w', 'h', 'x1', 'y1', 'image_id_orig', 'image_path', 'cluster_id'])\n",
    "    if combine_with_train:\n",
    "        s1_df = pd.concat([s1_df, train_df])\n",
    "    return s1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_round(num, model):\n",
    "    s1_df = get_pseudo_labels(combine_with_train=False)\n",
    "    \n",
    "    for cluster_id in range(N_CLUSTERS):\n",
    "        train_s1_df = s1_df[s1_df['cluster_id'] == cluster_id]\n",
    "        train_dataset = WheatTrainDataset(train_s1_df, get_train_transforms())\n",
    "        train_data_loader = DataLoader(train_dataset, batch_size=1, num_workers=4, drop_last=False, collate_fn=collate_fn)\n",
    "\n",
    "        model.load_state_dict(torch.load(weights_path))\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001)\n",
    "        \n",
    "        for epoch in range(PL_EPOCHS):\n",
    "            model.train()\n",
    "            for step, (images, targets, image_ids) in enumerate(train_data_loader):\n",
    "                # Load images/targets to cuda\n",
    "                images = [img.cuda() for img in images]\n",
    "                targets = [{k: v.cuda() for k, v in l.items()} for l in targets]\n",
    "                # Send images through network\n",
    "                features, detections, losses = model(images, targets)\n",
    "                # Optimizer step\n",
    "                optimizer.zero_grad()\n",
    "                loss_FS = sum(losses.values())\n",
    "                loss_FS.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        torch.save(model.state_dict(), 'cluster_%d.pth' % cluster_id)\n",
    "    return s1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    }
   ],
   "source": [
    "for round_num in range(PL_ROUNDS):\n",
    "    s1_df = train_round(round_num, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Make Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prediction_string(boxes, scores):\n",
    "    pred_strings = []\n",
    "    for j in zip(scores, boxes):\n",
    "        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n",
    "\n",
    "    return \" \".join(pred_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['cluster_id'] = [id_to_cluster[iid] for iid in test_df['image_id'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_threshold = 0.5\n",
    "image_size = 1024\n",
    "results = []\n",
    "\n",
    "for cluster_id in range(N_CLUSTERS):\n",
    "    model.load_state_dict(torch.load('cluster_%d.pth' % cluster_id))\n",
    "    model.eval()\n",
    "\n",
    "    test_df_cluster = test_df[test_df['cluster_id'] == cluster_id]\n",
    "    test_dataset = WheatTestDataset(test_df_cluster, test_path)\n",
    "    test_data_loader = DataLoader(test_dataset, batch_size=1, num_workers=4, drop_last=False, collate_fn=collate_fn)\n",
    "    \n",
    "    for images, image_ids in test_data_loader:\n",
    "        image_id = image_ids[0]\n",
    "    \n",
    "        predictions = []\n",
    "        for img in images[0]:\n",
    "            inp = [torch.tensor(img).cuda().permute(2,0,1)]\n",
    "            features, pred, loss = model(inp)\n",
    "            predictions.extend(pred)\n",
    "\n",
    "        boxes, scores, labels = run_wbf(predictions, image_size=image_size)\n",
    "        boxes = boxes.astype(np.int32).clip(min=0, max=int(image_size-1))\n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "\n",
    "        result = {\n",
    "            'image_id': image_id,\n",
    "            'PredictionString': format_prediction_string(boxes, scores)\n",
    "        }\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aac893a91</td>\n",
       "      <td>0.9994 558 533 125 188 0.9993 245 86 132 146 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51f1be19e</td>\n",
       "      <td>0.9993 610 88 152 168 0.9987 71 692 126 216 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51b3e36ab</td>\n",
       "      <td>0.9999 543 30 248 129 0.9999 688 612 334 130 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>348a992bb</td>\n",
       "      <td>0.9998 733 223 141 88 0.9998 597 444 123 97 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cc3532ff6</td>\n",
       "      <td>0.9999 768 828 168 165 0.9998 472 404 127 151 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id                                   PredictionString\n",
       "0  aac893a91  0.9994 558 533 125 188 0.9993 245 86 132 146 0...\n",
       "1  51f1be19e  0.9993 610 88 152 168 0.9987 71 692 126 216 0....\n",
       "2  51b3e36ab  0.9999 543 30 248 129 0.9999 688 612 334 130 0...\n",
       "3  348a992bb  0.9998 733 223 141 88 0.9998 597 444 123 97 0....\n",
       "4  cc3532ff6  0.9999 768 828 168 165 0.9998 472 404 127 151 ..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
